### Tutorials
DNA Nexus provides cloud computing infrastructure and large-scale data analysis for the UK Biobank (and other datasets) by acting as an intermediary between the user and AWS EC2 (compute) and AWS S3 (storage). The organization has a Youtube channel which provides tutorials. I have included a few helpful videos here, but there are many others online. DNA Nexus also holds frequent workshops as well as customer support.

- UK Biobank (UKB) Research Analysis Platform (RAP) Access and Setup
  - https://youtube.com/watch?v=RZPMmobKnTw&si=EnSIkaIECMiOmarE&t=2315

- R Studio Workbench on RAP
  - https://www.youtube.com/watch?v=iy22sxlj5Ik&list=PLRkZ0Fz-n3Z7Jg0Vz4vudLYnBza4EUGLM&index=7

- Accessing Project Files Using DNA Nexus file system
  - https://www.youtube.com/watch?v=glIjnFxlw-I

### Platform Setup and Data Access
The DNA Nexus UKB RAP links directly with your UKB AMS account. The steps for platform setup and data access so you are able to start analysis is as follows:
1) Get access to UK Biobank dataset:
   - Submit a grant application, pay fees, to get access (_Winok to describe more_)
   - Leech off someone who already has access and add you to their application
   - Make an account on AMS (access management system): https://bbams.ndph.ox.ac.uk/ams
   - View "Applications" tab on the left hand side to see your projects and access data

2) Get access to UKB RAP:
   - Make an account on UKB RAP: https://ukbiobank.dnanexus.com/landing
   - Do NOT make an account on DNA Nexus homepage (https://platform.dnanexus.com/) as this is the general account and not the UKB platform

3) Connect UKB RAP to AMS:
   - Once you have finished the UKB RAP account sign up process, you will get an email activation
   - Click the link in the email to be prompted back into UKB RAP
   - Once there, you will be automatically prompted to sign-in to your UKB AMS account through their portal
   - Enter your AMS details to link your AMS and UKB RAP accounts
 
4) Data access:
   - Once you're logged in and everything is linked, navigate to the "Project" tab on the top left
   - Click the green "New Project" button on the top right
   - Enter a name for your project (date by default)
   - Under the "UK Biobank" section, enter the application ID that matches that of the ID displayed on your AMS "Applications" section
   - Tick the "Dispense data to the project" box
   - Select the billing to your default account
   - Click "Create Project" to finish
   - Your UKB RAP "Projects" section should now have a new row showing the project you just created which is attached to your UKB application (AMS) as well as a "Status" column showing the progress of the data being loaded (usually takes a day)
   - Once the data has finished dispensing, the status will change to "Ready" which means you are ready to start analysis
   - You can refreash your project data for updates by going to "settings" tab and scrolling down to "UK Biobank" > "Check for Updates"

![image](https://user-images.githubusercontent.com/29684281/214428148-5b88b90a-bcd7-4824-9dac-11f95259d8e4.png)

5) Starting a R studio session:
    - Click on the 2nd tab on the top left "Tools" > "R Studio" > "New R Studio"
    - In pop up window, under project, select the project which contains the data you want to analyze, then click "start environment"
    - Your R studio session will initialize and then launch, the "Status" column will say "Ready" in green when its ready for use
    - Click on the name of your session to be forwarded to a new window, then click "New Session" > "Start Session" and you will see the familiar R studio integrated development environment (IDE)
    - Your session will be started in a "Cluster" environment, which means that it is an isolated environment with a specific set of memory and computational resource
    - This is a brand new environment created by AWS EC2 which contains nothing, which means you have to set up your entire pipeline from scratch everytime
    - This can be easily done with bash scripts to pull code from github, install all dependencies, and run the pre-defined code
    - Once you are done, click the terminate session on the right hand side of the row to close the session

6) Starting R studio session from project (similar to above):
    - Click into the project you want to perform analysis for in the "Projects" tab of your UKB RAP homepage
    - You will then see a file system of all the data associated with this project application
    - You can navigate in the directories to get a feel for the structure of the data
    - You can also open and download individual files and look at them on your own device
    - To use code to perform analysis, click the green "Start Analysis" button, scroll down until you see "R Studio Workbench", then click "Run selected"
    - This will lead you to a new pop up window where you can configure your session, then click "start analysis", this will initialize your R studio session for the selected project
    - Wait for it to load a little while, and click the "worker URL" along the top to enter into the session, then you can set up your R studio similarily as above
    - You can monitor all your sessions with the "Monitor" tab which summarizes all your active/terminated sessions, here you can also access your R studio by clicking link in each row corresponding to your active sesssion

7) Data Analysis in R:
    - The environment created here is an "instance" on the cloud and has no resources inside apart from R
    - To access project data files, we need to use the ```dx``` package provided by UKB RAP, which works in the terminal
    - ```dx ls``` will list all the data resources in your project, e.g. ```dx ls Bulk``` lists all the bulk data files
    - Alternatively, you can access the files directly, files for each project are all stored in the path ```/mnt/project/```, which will lead to you to see everything you could see interactively in the GUI filesystem when you first click into your project
      - Create a new file in R studio (top bar): File > New file > R script, save it with some filename "temp.R"
      - Make sure your working directory is the same as where the file is (top bar): Session > Set Working Directory > To Source File Location
      - Or you can use the R function ```setwd("/my/path")``` and absolute path to automatically set the workdir every time
      - Example of reading in a file with R: ```df = read.delim("/mnt/project/Showcase metadata/field.tsv")```
      - Of course, you can access all the other files as well in a similar manner (using different read functions for different file formats) for any file located in the ```/mnt/project``` directory
      - Imaging data (Bulk data) are located in ```/mnt/project/Bulk```
      - e.g. the terminal command ```ls -l /mnt/project/Bulk/Heart MRI/CINE tagging/10``` lists all cine MRIs for patients starting with the "10" ID
      - ```ls /mnt/project/Bulk/Heart MRI/CINE tagging/10 | wc -l``` counts the number of files in that directory
      - Once you have the data loaded into your R studio workspace, then you go about performing whatever analysis you choose
    - Lets say you have performed some analysis, and you want to save your output (stored as the variable ```df_out```) to your own computer
      - Using the previous file loaded, you can perform ```df_out = df[df$availability == 4, ]``` to make some mock subset of the data
      - ```write.csv(df_out, file = "my_output.csv")``` will write this file to inside your R studio instance to your current working directory
      - ```system("dx upload my_output.csv")``` will then upload this file to the ```/mnt/project``` path where it will be accessible in the projects filesystem GUI
      - Make sure you run this command inside the working directory which contains the ```my_output.csv``` file
      - ```/mnt/project``` is read-only so you have to use ```dx``` to upload any files
      - ```/mnt/project``` is automatically the base path for all ```dx``` commands, so any further path would stem from this
      - e.g. ```system("dx upload my_output.csv --path . --destination /Bulk/")``` will upload ```my_output.csv``` located in ```.``` to ```/mnt/project/Bulk```
      - You can of course perform similar code for image analysis located in ```mnt/project/Bulk``` with different functions
      - It is recommended you store all your code ina Github repo so you can pull it into your R studio workbench instance everytime
    - Terminate your instance with the red buttom on the top right corner
 
8) Full code for the example above (load, process, write, upload):
```
df = read.delim("/mnt/project/Showcase metadata/field.tsv")
df_out = df[df$availability == 4, ]
write.csv(df_out, file = "my_output.csv")
system("dx upload my_output.csv --path . --destination /Bulk/")
```

9) Exploring Tabular Data (Imaging-Derived Phenotypes - IDPs):
    - Tabular data (or table data) is located in the root directory of your project in the file "app<APPLICATION-ID>_<CREATION-TIME>"
    - You can browse this table using the UKB RAP cohort browser by clicking the "app<APPLICATION-ID>_<CREATION-TIME>.dataset" (.dataset suffix)
    - After clicking this file, a new window will pop up, where you can perform some exploratory analysis with the "Add Tile" button on the right hand side
    - A pop up side bar will then be revealed where you can then select which variable you want using the presented nested file organization system
    - You can choose to display this data as a histogram or bar plot, this will produce a new tile on your analysis interface and will show a plot and summary of the column/variable you selected
    - On the top, you can also apply filters with the "Add Filter" button to filter out unwanted rows
    - Once these tiles are created, you can click the "i" icon at the top of each tile which will bring up more options such as adding the data to the preview tab, or applying filters on individual tiles (variables)
    - To perform multiple plots, you can use "Add Tile" button to choose a first variable. Once you have the first variable open, you can then navigate to another variable, and instead of clicking on it, you can click the "+" button which will import the 2nd variable into the analysis panel, you can then play around with the settings to get different plot types, orientation, grouping, etc etc and then you can add it to your analysis interface with the "Add Tile" button
    - To save this analysis "board" or interface, you can click "save" icon on the top right, name your analysis, and it will be saved to your directory where you can then come back to view it later
    - Overall, the UKB RAP allows interactive visualization of the data available for quick analysis using the platform's computational resources. You can also make nice looking interactive visualization dashboards
    - More extensive details can be found here: https://documentation.dnanexus.com/user/cohort-browser

10) Downloading Tabular Data (IDPs):
    - The table data is very big so requires some more clever tools to analyze
    - Go to "Tools" tab on the top and click JupyterLab > New JupyterLab > Select your project > Spark Cluster as cluster configuration, and keep all else as default
    - Wait until it finishes initializing and launches
    - Select New Notebook, name it, open it, and select the python 3 kernel
    - You can navigate tools/options on the left side panel and top panel
    - Along the top of the notebook, you have options to run, stop, restart session, etc
    - The code below performs extraction, copy paste it into your jupyter notebook and run this code to save the UKB dataset into a .csv file
```
# import libraries we need to use
import os
import dxpy
import dxdata
from distutils.version import LooseVersion

# automatically find the ID of your current dataset (dont have to manually define the app*.database file every time)
dispensed_database_name = dxpy.find_one_data_object(classname = "database", name = "app*", folder = "/", name_mode = "glob", describe=True)["describe"]["name"]
dispensed_dataset_id = dxpy.find_one_data_object(typename = "Dataset", name = "app*.dataset", folder = "/", name_mode = "glob")["id"]

# load dataset containing the main UKB participant data
dataset = dxdata.load_dataset(id = dispensed_dataset_id)["participant"]

# define function which helps us retrieve all column names, get all columns by using empty string 
#def fields_by_title_keyword(keyword):
#    fields = list(dataset.find_fields(lambda f: keyword.lower() in f.title.lower()))
#    return sorted(fields, key = lambda f: LooseVersion(f.name))
#
#all_colnames = [f.name for f in fields_by_title_keyword("")]

# Returns all field names for a given UKB showcase field id
def fields_for_id(field_id):
    field_id = str(field_id)
    fields = dataset.find_fields(name_regex=r'^p{}(_i\d+)?(_a\d+)?$'.format(field_id))
    return sorted(fields, key = lambda f: LooseVersion(f.name))
def field_names_for_id(field_id):
    return [f.name for f in fields_for_id(field_id)]

# define all columns we are interested in (variables manually defined previously with code in R)
field_ids = [12292,12336,12338,12340,12673,12679,12682,12685,12686,12702,
             20150,20151,20153,20154,20156,20258,21003,
             22330,22332,22333,22334,22335,22336,22337,22338,22400,22401,22402,22403,22404,22405,22406,22407,22408,22409,22410,22420,22421,22423,22424,22425,22426,22427,22670,22671,22672,22673,22674,22675,22676,22677,22678,22679,22680,22681,22682,
             23098,23099,23100,23101,23102,23104,23106,23107,23108,23109,23110,23111,23112,23113,23114,23115,23116,23117,23118,23119,23120,23121,23122,23123,23124,23125,23126,23127,23128,23129,23130,23200,23201,23202,23203,23204,23205,23206,23208,23209,23210,23212,23213,23214,23215,23216,23217,23218,23219,23220,23221,23222,23223,23224,23225,23226,23227,23228,23229,23230,23231,23232,23233,23234,23235,23236,23237,23238,23239,23240,23241,23242,23243,23244,23245,23246,23247,23248,23249,23250,23251,23252,23253,23254,23255,23256,23257,23258,23259,23260,23261,23262,23263,23264,23265,23266,23267,23268,23269,23270,23271,23272,23273,23274,23275,23276,23277,23278,23279,23280,23281,23282,23283,23284,23285,23286,23287,23288,23289,23290,23291,23292,23293,23295,23296,23297,23298,23299,23300,23301,23302,23304,23305,23306,23307,23308,23309,23310,23311,23312,23313,23314,23315,23316,23317,23318,23320,
             25000,25001,25002,25003,25004,25005,25006,25007,25008,25009,25010,25011,25012,25013,25014,25015,25016,25017,25018,25019,25020,25021,25022,25023,25024,25025,25026,25027,25028,25029,25030,25031,25032,25033,25034,25035,25036,25037,25038,25039,25056,25057,25058,25059,25060,25061,25062,25063,25064,25065,25066,25067,25068,25069,25070,25071,25072,25073,25074,25075,25076,25077,25078,25079,25080,25081,25082,25083,25084,25085,25086,25087,25088,25089,25090,25091,25092,25093,25094,25095,25096,25097,25098,25099,25100,25101,25102,25103,25104,25105,25106,25107,25108,25109,25110,25111,25112,25113,25114,25115,25116,25117,25118,25119,25120,25121,25122,25123,25124,25125,25126,25127,25128,25129,25130,25131,25132,25133,25134,25135,25136,25137,25138,25139,25140,25141,25142,25143,25144,25145,25146,25147,25148,25149,25150,25151,25152,25153,25154,25155,25156,25157,25158,25159,25160,25161,25162,25163,25164,25165,25166,25167,25168,25169,25170,25171,25172,25173,25174,25175,25176,25177,25178,25179,25180,25181,25182,25183,25184,25185,25186,25187,25188,25189,25190,25191,25192,25193,25194,25195,25196,25197,25198,25199,25200,25201,25202,25203,25204,25205,25206,25207,25208,25209,25210,25211,25212,25213,25214,25215,25216,25217,25218,25219,25220,25221,25222,25223,25224,25225,25226,25227,25228,25229,25230,25231,25232,25233,25234,25235,25236,25237,25238,25239,25240,25241,25242,25243,25244,25245,25246,25247,25248,25249,25250,25251,25252,25253,25254,25255,25256,25257,25258,25259,25260,25261,25262,25263,25264,25265,25266,25267,25268,25269,25270,25271,25272,25273,25274,25275,25276,25277,25278,25279,25280,25281,25282,25283,25284,25285,25286,25287,25288,25289,25290,25291,25292,25293,25294,25295,25296,25297,25298,25299,25300,25301,25302,25303,25304,25305,25306,25307,25308,25309,25310,25311,25312,25313,25314,25315,25316,25317,25318,25319,25320,25321,25322,25323,25324,25325,25326,25327,25328,25329,25330,25331,25332,25333,25334,25335,25336,25337,25338,25339,25340,25341,25342,25343,25344,25345,25346,25347,25348,25349,25350,25351,25352,25353,25354,25355,25356,25357,25358,25359,25360,25361,25362,25363,25364,25365,25366,25367,25368,25369,25370,25371,25372,25373,25374,25375,25376,25377,25378,25379,25380,25381,25382,25383,25384,25385,25386,25387,25388,25389,25390,25391,25392,25393,25394,25395,25396,25397,25398,25399,25400,25401,25402,25403,25404,25405,25406,25407,25408,25409,25410,25411,25412,25413,25414,25415,25416,25417,25418,25419,25420,25421,25422,25423,25424,25425,25426,25427,25428,25429,25430,25431,25432,25433,25434,25435,25436,25437,25438,25439,25440,25441,25442,25443,25444,25445,25446,25447,25448,25449,25450,25451,25452,25453,25454,25455,25456,25457,25458,25459,25460,25461,25462,25463,25464,25465,25466,25467,25468,25469,25470,25471,25472,25473,25474,25475,25476,25477,25478,25479,25480,25481,25482,25483,25484,25485,25486,25487,25488,25489,25490,25491,25492,25493,25494,25495,25496,25497,25498,25499,25500,25501,25502,25503,25504,25505,25506,25507,25508,25509,25510,25511,25512,25513,25514,25515,25516,25517,25518,25519,25520,25521,25522,25523,25524,25525,25526,25527,25528,25529,25530,25531,25532,25533,25534,25535,25536,25537,25538,25539,25540,25541,25542,25543,25544,25545,25546,25547,25548,25549,25550,25551,25552,25553,25554,25555,25556,25557,25558,25559,25560,25561,25562,25563,25564,25565,25566,25567,25568,25569,25570,25571,25572,25573,25574,25575,25576,25577,25578,25579,25580,25581,25582,25583,25584,25585,25586,25587,25588,25589,25590,25591,25592,25593,25594,25595,25596,25597,25598,25599,25600,25601,25602,25603,25604,25605,25606,25607,25608,25609,25610,25611,25612,25613,25614,25615,25616,25617,25618,25619,25620,25621,25622,25623,25624,25625,25626,25627,25628,25629,25630,25631,25632,25633,25634,25635,25636,25637,25638,25639,25640,25641,25642,25643,25644,25645,25646,25647,25648,25649,25650,25651,25652,25653,25654,25655,25656,25657,25658,25659,25660,25661,25662,25663,25664,25665,25666,25667,25668,25669,25670,25671,25672,25673,25674,25675,25676,25677,25678,25679,25680,25681,25682,25683,25684,25685,25686,25687,25688,25689,25690,25691,25692,25693,25694,25695,25696,25697,25698,25699,25700,25701,25702,25703,25704,25705,25706,25707,25708,25709,25710,25711,25712,25713,25714,25715,25716,25717,25718,25719,25720,25721,25722,25723,25724,25725,25726,25727,25728,25729,25730,25731,25732,25733,25734,25735,25736,25738,25781,25782,25783,25784,25785,25786,25787,25788,25789,25790,25791,25792,25793,25794,25795,25796,25797,25798,25799,25800,25801,25802,25803,25804,25805,25806,25807,25808,25809,25810,25811,25812,25813,25814,25815,25816,25817,25818,25819,25820,25821,25822,25823,25824,25825,25826,25827,25828,25829,25830,25831,25832,25833,25834,25835,25836,25837,25838,25839,25840,25841,25842,25843,25844,25845,25846,25847,25848,25849,25850,25851,25852,25853,25854,25855,25856,25857,25858,25859,25860,25861,25862,25863,25864,25865,25866,25867,25868,25869,25870,25871,25872,25873,25874,25875,25876,25877,25878,25879,25880,25881,25882,25883,25884,25885,25886,25887,25888,25889,25890,25891,25892,25893,25894,25895,25896,25897,25898,25899,25900,25901,25902,25903,25904,25905,25906,25907,25908,25909,25910,25911,25912,25913,25914,25915,25916,25917,25918,25919,25920,
             30000,30010,30020,30030,30040,30050,30060,30070,30080,30090,30100,30110,30120,30130,30140,30150,30160,30170,30180,30190,30200,30210,30220,30230,30240,30250,30260,30270,30280,30290,30300,30500,30510,30520,30530,30600,30610,30620,30630,30640,30650,30660,30670,30680,30690,30700,30710,30720,30730,30740,30750,30760,30770,30780,30790,30800,30810,30820,30830,30840,30850,30860,30870,30880,30890,31,
             4079,4080,43,54,
             6150,6153,6177,6218,6219,6220,6221,6222]

# use column query function to return and collate all columns with matching substring IDs
all_colnames = sum([field_names_for_id(field_id) for field_id in field_ids], [])

# insert patient id in first column 
all_colnames.insert(0, "eid")

# retrive columns from database, convert to pandas, change column names to original UKB format
df = dataset.retrieve_fields(names = all_colnames, engine = dxdata.connect())
df = df.toPandas()
print("Dataframe Shape is: " + str(df.shape[0]) + " rows by " + str(df.shape[1]) + " columns")

# write to output, move from this temporary instance to our project repo 
df.to_csv('ukb_' + dispensed_database_name + '.csv', index = False)
print("Locally Written File Name: " + "ukb_" + dispensed_database_name + ".csv")

# move this file from this environment into projects folder where we can see it in the UKB RAP interface
os.system("dx upload " + "ukb_" + dispensed_database_name + ".csv" + " --dest /")
print("Transfered from Local Instance into Projects")
```
  - once this code is ran, you can see the new file output in the "folder" symbol on the left hand side tab at the very top, you can right click, download to download it straight onto your desktop
  - If you navigate back to your "project" tab on your main UKB RAP homepage and go into your project, you can also see this new .csv file there, and you can download it from there too
  - More documentation can be found at: https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data#launching-jupyterlab
  - Here is a demo which is fairly nice too: https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb

Other Notes:
  - The longer you leave your instance on, the more it costs, so best automate everything with linux commands and be in-and-out quick
  - Your computational speed and memory size will be limited to the instance type you choose, where larger instances costs more money to run
  - This is a very nice exploratory tool to look at what data is available, and plan how you can next tackle this dataset
  - The two projects shown above have 6,200,00 GB and 190,000 GB of data respectively
  - Downloading data from the platform to your local device is fairly expensive when in Bulk (£0.04 per GB)
  - You could try writing python scripts to subset the bulk data, store it in the project directory temporarily, and then download it, although this would be expensive if you need to do it repeatedly
