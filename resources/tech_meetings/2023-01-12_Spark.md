### Spark
- Apache spark is an open-source unified analytics engine for large-scale data processing
- Spark provides an interface for programming clusters with implicity data parallelism and fault tolerance
- A SparkDataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database or data frame, but with richer optimizations under the hood

### Spark in Python
- PySpark is the library for using Spark in Python
- It supports transitions from Pandas to PySpark DataFrames
- Pandas support CSV, JSON, SQL and many others by creating a dataframe containing rows and columns
- However, pandas does not support distributed processing, hence you would always need to increase the resources when increasing data
- Here is an example of a basic pandas data frame
```
import pandas as pd    
data = [["James","","Smith",30,"M",60000], 
        ["Michael","Rose","",50,"M",70000], 
        ["Robert","","Williams",42,"",400000], 
        ["Maria","Anne","Jones",38,"F",500000], 
        ["Jen","Mary","Brown",45,None,0]] 
columns = ['First Name','Middle Name','Last Name','Age','Gender','Salary']

# Create the pandas DataFrame 
pandasDF = pd.DataFrame(data=data, columns=columns) 
  
# print dataframe/perform some summaries
print(pandasDF)
print(pandasDF.count())
print(pandasDF.max())
print(pandasDF.mean())
```
- Pandas run operations on a single machine whereas PySpark runs on multiple machines, hence it can scale up extremely quickly
- PySpark supports:
    - In-memory computation
    - Distributed processing using parallelize
    - Used across many cluster managers
    - Immutable (cannot be changed once created)
    - Fault-tolerant
    - Transformations with lazy evaluation (not executed until actions are called)
    - In-built optimization
- PySpark is excellent for data ingestion, and is compatiable with Hadoop HDFS, AWS S3, etc
- PySpark has the following modules:
    - PySpark RDD (resilient distributed dataset)
    - PySpark DataFrame and SQL
    - PySpark Streaming
    - PySpark MLib (machine learning library)
    - PySpark GraphFrames
    - PySpark Resource
- An example of PySpark DataFrame is as follows:
```
from pyspark.sql import SparkSession
# Create SparkSession
spark = SparkSession.builder.appName('Example name').getOrCreate()

data = [("James","","Smith",30,"M",60000),
        ("Michael","Rose","",50,"M",70000),
        ("Robert","","Williams",42,"",400000),
        ("Maria","Anne","Jones",38,"F",500000),
        ("Jen","Mary","Brown",45,"F",0)]

columns = ["first_name","middle_name","last_name","Age","gender","salary"]
pysparkDF = spark.createDataFrame(data = data, schema = columns)
pysparkDF.printSchema()
pysparkDF.show(truncate=False)
```
- You can also read a csv file into PySpark with:
```
df = spark.read.csv("file.csv", sep=',', inferSchema=True, header=True)
```
- With SQL, you can perform (dataframe from above)
```
pysparkDF.groupBy("gender") \
         .agg(mean("age"),mean("salary"),max("salary")) \
         .show()

pysparkDF.createOrReplaceTempView("Employee")
spark.sql("select * from Employee where salary > 100000").show()
```
- Creating PySpark DataFrame from Pandas
```
pysparkDF2 = spark.createDataFrame(pandasDF) 
pysparkDF2.printSchema()
pysparkDF2.show()
```
- Convert back to pandas
```
pandasDF = pysparkDF.toPandas()
```

### Spark in R
- sparkR is the library for using Spark in the R programming language
- The entry point into SparkR is the SparkSession which connects your R program to a Spark cluster
```
sparkR.session()
```
- Make sure SPARK_HOME is set in environment by checking it with an if statement
- Then you can load the library
- Start the session by defining configurations as input
```
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
  Sys.setenv(SPARK_HOME = "/home/spark")
}
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
```
- Once you are in a SparkSession, you can create SparkDataFrames from R data frames with ```as.DataFrame``` or ```createDataFrame```
```
df = as.DataFrame(df)
head(df) # view first few rows of SparkDataFrame
```
- To read SparkDataFrames you can use ```read.df``` which supports reading JSON or CSV files
```
df = read.df("example.csv", "csv", header = "true", inferSchema = "true", na.strings = "NA")
head(df)
printSchema(df) # view the column names and properties
```
- We can also write SparkDataFrames to file using ```write.df```
- To manipulate SparkDataFrames, SparkR has many useful functions
```
select(df, df$col1) # only select 1 column
select(df, "col1") # only select 1 column
filter(df, df$col2 > 100) # row subsetting
summarize(groupBy(df, df$col1), count = n(df$col1)) # aggregation
arrange(df, desc(df$col1)) # sorting by column
agg(cube(df, "col1", "col2", "col3"), avg(df$col4)) # manipulate multiple columns
agg(rollup(df, "col1", "col2", "col3"), avg(df$col4)) # roll up to summarize by unique col1-col3
df$col1 = df$col1 * 10 # operate on columns
```
- Apply functions can also be used in SparkR
```
schma = structType(structField("col1", "double"), structField("col2", "double"))
df = dapply(df, function(x) x = x$col1 * 10, schema)

df = dapplyCollect(df, function(x) x = x$col1 * 10) # same as above but does not need schema input, however this can fail as we are not subsetting the dataframe and it could run into memory errors

df = gapply(df, "col1", function(key, x) y = data.frame(key, max(x$col2)), schema) # apply function to each group of the SparkDataFrame

df = gapplyCollect(df, "col1", function(key, x) y = data.frame(key, max(x$col2))) # same as above
```
- If you would like to use R functions, you can do so with ```spark.lapply(df, funct)```
- SQL queries can be ran with
```
df = read.df("file.csv", "csv")
createOrReplaceTempView(df, "df") # register this SparkDataFrame as temporary view
df_select = sql("SELECT val1 FROM col1 WHERE col2 > 10")
```
- SparkR also supports various machine learning algorithms (not limited to):
    - Logistic Regression/MLP/Naive Bayes, SVM
    - GLM/Linear Regression
    - RF/Decision Trees
    - K-Means/GMM
- When using SparkR, it is possible to run into conflicts with the name function name
- In this scenario it is vest to use the native R convention of calling functions from specific libraries
```
stats::cov(x, y) # SparkR has its own cov function which masks out the R base function
stats::filter(x, filter) # same thing as above
stats::sample(x, size) # same thing as above
```
