### Spark
- Apache spark is an open-source unified analytics engine for large-scale data processing
- Spark provides an interface for programming clusters with implicity data parallelism and fault tolerance
- A SparkDataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database or data frame, but with richer optimizations under the hood

### Spark R
- sparkR is the library for using Spark in the R programming language
- The entry point into SparkR is the SparkSession which connects your R program to a Spark cluster
```
sparkR.session()
```
- Make sure SPARK_HOME is set in environment by checking it with an if statement
- Then you can load the library
- Start the session by defining configurations as input
```
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
  Sys.setenv(SPARK_HOME = "/home/spark")
}
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
```
- Once you are in a SparkSession, you can create SparkDataFrames from R data frames with ```as.DataFrame``` or ```createDataFrame```
```
df = as.DataFrame(df)
head(df) # view first few rows of SparkDataFrame
```
- To read SparkDataFrames you can use ```read.df``` which supports reading JSON or CSV files
```
df = read.df("example.csv", "csv", header = "true", inferSchema = "true", na.strings = "NA")
head(df)
printSchema(df) # view the column names and properties
```
- We can also write SparkDataFrames to file using ```write.df```
- To manipulate SparkDataFrames, SparkR has many useful functions
```
select(df, df$col1) # only select 1 column
select(df, "col1") # only select 1 column
filter(df, df$col2 > 100) # row subsetting
summarize(groupBy(df, df$col1), count = n(df$col1)) # aggregation
arrange(df, desc(df$col1)) # sorting by column
agg(cube(df, "col1", "col2", "col3"), avg(df$col4)) # manipulate multiple columns
agg(rollup(df, "col1", "col2", "col3"), avg(df$col4)) # roll up to summarize by unique col1-col3
df$col1 = df$col1 * 10 # operate on columns
```
- Apply functions can also be used in SparkR
```
schma = structType(structField("col1", "double"), structField("col2", "double"))
df = dapply(df, function(x) x = x$col1 * 10, schema)

df = dapplyCollect(df, function(x) x = x$col1 * 10) # same as above but does not need schema input, however this can fail as we are not subsetting the dataframe and it could run into memory errors

df = gapply(df, "col1", function(key, x) y = data.frame(key, max(x$col2)), schema) # apply function to each group of the SparkDataFrame

df = gapplyCollect(df, "col1", function(key, x) y = data.frame(key, max(x$col2))) # same as above
```
- If you would like to use R functions, you can do so with ```spark.lapply(df, funct)```
- SQL queries can be ran with
```
df = read.df("file.csv", "csv")
createOrReplaceTempView(df, "df") # register this SparkDataFrame as temporary view
df_select = sql("SELECT val1 FROM col1 WHERE col2 > 10")
```
- SparkR also supports various machine learning algorithms (not limited to):
    - Logistic Regression/MLP/Naive Bayes, SVM
    - GLM/Linear Regression
    - RF/Decision Trees
    - K-Means/GMM
- When using SparkR, it is possible to run into conflicts with the name function name
- In this scenario it is vest to use the native R convention of calling functions from specific libraries
```
stats::cov(x, y) # SparkR has its own cov function which masks out the R base function
stats::filter(x, filter) # same thing as above
stats::sample(x, size) # same thing as above
```

### Spark Python
- 
