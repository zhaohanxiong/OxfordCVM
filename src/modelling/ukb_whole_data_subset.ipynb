{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Code to Preprocess UKB Raw Dataset"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["22/12/16 15:26:33 WARN Utils: Your hostname, ccrfai resolves to a loopback address: 127.0.1.1; using 163.1.212.155 instead (on interface eno8303)\n","22/12/16 15:26:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["22/12/16 15:26:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["# import library\n","import pyspark\n","from pyspark.sql import SparkSession\n","\n","# define data path\n","path_in = \"../../../ukb51139_v2.csv\"\n","path_out = \"../../../ukb_subset_v2.csv\"\n","\n","# set up PySpark session\n","spark = SparkSession.builder.master('local[*]') \\\n","                            .config(\"spark.driver.memory\", \"50g\") \\\n","                            .appName(\"UKB\").getOrCreate()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Load, Manipulate, Save Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+-------+-------+\n","|X4080.2.0|X4079.2.0|X93.2.0|X94.2.0|\n","+---------+---------+-------+-------+\n","|      116|       58|   null|   null|\n","|      157|      108|   null|   null|\n","|      151|       86|   null|   null|\n","|     null|     null|    139|     82|\n","|      144|       83|   null|   null|\n","|      152|       76|   null|   null|\n","|      145|       75|   null|   null|\n","|      140|       74|   null|   null|\n","|      161|       92|   null|   null|\n","|      112|       62|   null|   null|\n","+---------+---------+-------+-------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------+\n","|count(1)|\n","+--------+\n","|   46211|\n","+--------+\n","\n","22/12/16 15:29:06 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Load ukb as spark data frame\n","df = spark.read.csv(path_in, header=True)\n","\n","# Filter missing blood pressure rows\n","df = df.filter(\"(`X4080.2.0` is NOT NULL OR `X93.2.0` is NOT NULL) AND \\\n","                (`X4079.2.0` is NOT NULL OR `X94.2.0` is NOT NULL)\")\n","\n","# Show blood pressure columns\n","df.select(df[\"`X4080.2.0`\"], df[\"`X4079.2.0`\"], \n","          df[\"`X93.2.0`\"], df[\"`X94.2.0`\"]).limit(10).show()\n","\n","# Save data to file for analysis somewhere else\n","df.write.csv(path_out)\n","\n","# register data frame as a sql table to run queries\n","df.registerTempTable('ukb')\n","\n","# run SQL queries\n","#spark.sql('SELECT COUNT(*) FROM ukb').show()\n","#spark.sql('SELECT COUNT(*) FROM ukb \\\n","#           WHERE `X4080.2.0` > 160 AND `X4079.2.0` > 100').show()"]}],"metadata":{"kernelspec":{"display_name":"env_conda","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"951ad22e190c25c83794d47e80d11a929decfd8e6fb20f76464f29d425a548ff"}}},"nbformat":4,"nbformat_minor":2}
